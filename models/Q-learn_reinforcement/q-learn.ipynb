{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be022ec-c0fd-494f-b3dc-67a211d499a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea953c87-a120-480e-9d77-71497a1b5881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- ENVIRONMENT -----\n",
    "\n",
    "# 5 states (0 to 4)\n",
    "# State 4 is final goal of model\n",
    "n_states = 5\n",
    "goal_state = 4\n",
    "\n",
    "# Actions: 0 = move left, 1 = move right\n",
    "n_actions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f52dd1-b16b-4c3d-b9a4-c40bba802f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards\n",
    "# Reaching goal gives +10, otherwise 0\n",
    "def get_reward(state, action):\n",
    "    if state == 3 and action == 1:\n",
    "        return 10  # moving right from state 3 -> state 4 (goal)\n",
    "    return 0\n",
    "\n",
    "# Next state logic\n",
    "def get_next_state(state, action):\n",
    "    if action == 0:  # left\n",
    "        return max(0, state - 1)\n",
    "    else:            # right\n",
    "        return min(4, state + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4eda8f3-0fea-4c79-ab59-bc977291a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Q-LEARNING PARAMETERS -----\n",
    "alpha = 0.1        # learning rate: How fast it learns\n",
    "gamma = 0.9        # discount factor: How much the agent cares about future rewards vs immediate rewards.\n",
    "epsilon = 0.2      # exploration chance : Chance to try new actions\n",
    "episodes = 200     # How many times the agent repeats the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed2861f-d557-4948-8b80-00d41d828dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-table (remember the table of each action and state reward?)\n",
    "Q = np.zeros((n_states, n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf0ea87-09f0-4baa-9522-9e447858c216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Final Q-Table:\n",
      "[[5.53074977 7.28999726]\n",
      " [5.12627537 8.09999953]\n",
      " [6.15857297 8.99999993]\n",
      " [7.35433108 9.99999999]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# ----- TRAINING LOOP -----\n",
    "for ep in range(episodes):\n",
    "    state = 0  # start always at state 0\n",
    "\n",
    "    while state != goal_state:\n",
    "\n",
    "        # Decide an action (explore or exploit)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, n_actions - 1)  #exploration\n",
    "        else:\n",
    "            action = np.argmax(Q[state])  #exploit\n",
    "\n",
    "        reward = get_reward(state, action) # Get the reward\n",
    "        next_state = get_next_state(state, action) # Move to the next state\n",
    "\n",
    "        # Q-learning update (the LEARNING step)\n",
    "        Q[state][action] = Q[state][action] + alpha * (\n",
    "            reward + gamma * np.max(Q[next_state]) - Q[state][action]\n",
    "        )\n",
    "\n",
    "        state = next_state # Move to the next state\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(\"Final Q-Table:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc25e27-ea9d-42b7-a9be-d36e4d951ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned path: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# ----- TEST THE LEARNED POLICY -----\n",
    "state = 0  # starting position of the agent.\n",
    "path = [state]  # record every step the agent takes\n",
    "\n",
    "while state != goal_state: # This loop keeps running until the agent reaches the goal.\n",
    "    action = np.argmax(Q[state]) # This is the KEY part,It chooses the best possible action based on what the agent learned\n",
    "    state = get_next_state(state, action)\n",
    "    path.append(state) # add the new state to the path list.\n",
    "\n",
    "print(\"Learned path:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806dcd1-9cf7-4672-9061-158b2f9c1672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
